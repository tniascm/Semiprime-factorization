# Post-Quantum Cryptography Landscape

The ongoing transition from RSA and classical public-key cryptography to quantum-resistant alternatives, including standardization timelines, threat models, and the window of vulnerability.

---

### NIST Post-Quantum Cryptography Standards (FIPS 203, 204, 205)
- **Authors**: NIST (National Institute of Standards and Technology), with algorithms from various research teams
- **Year**: 2024 (finalized August 13, 2024)
- **Source**: [https://csrc.nist.gov/projects/post-quantum-cryptography](https://csrc.nist.gov/projects/post-quantum-cryptography)
- **Core Idea**: After an 8-year standardization process (launched 2016, initial submissions 2017, three rounds of evaluation), NIST finalized three post-quantum cryptographic standards. **ML-KEM (FIPS 203)** -- Module Learning with Errors Key Encapsulation Mechanism, based on CRYSTALS-Kyber -- provides quantum-resistant key exchange. **ML-DSA (FIPS 204)** -- Module Learning with Errors Digital Signature Algorithm, based on CRYSTALS-Dilithium -- provides quantum-resistant digital signatures. **SLH-DSA (FIPS 205)** -- Stateless Hash-Based Digital Signature Algorithm, based on SPHINCS+ -- provides a conservative, hash-based signature scheme as a fallback in case lattice-based assumptions are broken. All three are now official U.S. federal standards, mandated for government use and expected to become the global baseline.
- **Mathematical Foundation**: ML-KEM and ML-DSA are based on the Module Learning with Errors (MLWE) problem: given matrix A (random over Z_q) and vector b = A*s + e (where s is a secret and e is "small" error), find s. Security reduces to the hardness of lattice problems (approximate SVP on module lattices). SLH-DSA is based on hash function security: collision resistance, second preimage resistance, and properties of hash-based Merkle trees (XMSS-style hypertrees with FORS few-time signature scheme at the leaves). Key sizes: ML-KEM-768 has 1,184-byte public keys; ML-DSA-65 has 1,952-byte public keys; SLH-DSA-128s has 32-byte public keys but 7,856-byte signatures.
- **RSA Relevance**: These standards are the direct response to the threat that quantum computers (running Shor's algorithm) will break RSA. The existence of finalized standards means the cryptographic community has accepted that RSA's days are numbered. However, the transition timeline (see NIST IR 8547 entry) spans years to decades, during which RSA remains deployed and vulnerable. Every year the transition is delayed is a year of vulnerability. Conversely, if a classical factoring breakthrough occurred before quantum computers, it would validate the transition but also threaten systems that planned to delay migration.
- **Status**: Proven (standards finalized and published; cryptographic security based on well-studied hard problems)
- **Open Questions**: Will lattice-based assumptions hold against future algorithmic advances (both classical and quantum)? Are the chosen parameter sets conservative enough for 30+ year security? Will implementation vulnerabilities (side channels, fault attacks) undermine the theoretical security? How quickly will the ecosystem (TLS libraries, PKI infrastructure, embedded systems) actually adopt these standards?

---

### HQC Selected as Additional KEM Standard
- **Authors**: Carlos Aguilar Melchor, Nicolas Aragon, Slim Bettaieb, Loic Bidoux, Olivier Blazy, Jean-Christophe Deneuville, Philippe Gaborit, Edoardo Persichetti, Gilles Zemor, Jurjen Bos, Arnaud Dion
- **Year**: 2025 (selected March 11, 2025 for standardization)
- **Source**: [NIST PQC Round 4 selection](https://csrc.nist.gov/projects/post-quantum-cryptography)
- **Core Idea**: HQC (Hamming Quasi-Cyclic) is a code-based key encapsulation mechanism selected by NIST as an alternative to ML-KEM, providing algorithmic diversity. While ML-KEM is based on lattice problems, HQC is based on the hardness of decoding random quasi-cyclic codes -- a completely different mathematical foundation. This diversity is critical: if a breakthrough attack on lattice problems (MLWE) were discovered, HQC would remain secure, and vice versa. HQC uses a variant of the Niederreiter cryptosystem with quasi-cyclic structure for efficiency. Public key sizes are larger than ML-KEM (roughly 2-4x) but the scheme provides a genuinely independent security assumption.
- **Mathematical Foundation**: Quasi-cyclic syndrome decoding problem: given a random quasi-cyclic parity-check matrix H and syndrome s = H*e for a low-weight error vector e, find e. Security relies on the hardness of decoding random linear codes (an NP-hard problem in the worst case, with strong evidence for average-case hardness). Uses tensor product codes for error correction in decapsulation. The quasi-cyclic structure provides efficiency (compact representation of matrices) while maintaining security.
- **RSA Relevance**: HQC represents the cryptographic community's hedge against concentrated risk. If both RSA and lattice-based PQC share an unexpected vulnerability (e.g., if a hypothetical breakthrough in lattice algorithms also affected factoring-related problems), code-based cryptography provides a fallback. The selection of HQC also signals that NIST is planning for a world where even the first generation of PQC standards might need replacement -- a lesson learned from the decades-long RSA era.
- **Status**: Active standardization (selected, FIPS standard in development)
- **Open Questions**: Will HQC's larger key sizes limit adoption in constrained environments (IoT, embedded systems)? Are there undiscovered structural attacks on quasi-cyclic codes? How will HQC perform in hybrid deployments alongside ML-KEM? What is the concrete quantum query complexity for attacking HQC with Grover-enhanced decoding?

---

### FALCON: Fast-Fourier Lattice-Based Compact Signatures Over NTRU
- **Authors**: Pierre-Alain Fouque, Jeffrey Hoffstein, Paul Kirchner, Vadim Lyubashevsky, Thomas Pornin, Thomas Prest, Thomas Ricosset, Gregor Seiler, William Whyte, Zhenfei Zhang
- **Year**: 2017 (submitted to NIST), FIPS 206 in development (expected 2025-2026)
- **Source**: [https://falcon-sign.info/](https://falcon-sign.info/)
- **Core Idea**: FALCON is a lattice-based digital signature scheme based on the NTRU lattice and the GPV framework (Gentry, Peikert, Vaikuntanathan) for hash-and-sign signatures. It achieves the most compact signatures among the NIST PQC signature candidates (666 bytes for FALCON-512, compared to 2,420 bytes for ML-DSA-65), making it attractive for bandwidth-constrained applications. The key technique is Gaussian sampling over NTRU lattices using a fast Fourier "tree" structure (the "falcon" in the name stands for "FAst Fourier Lattice-based COmpact signatures over NTRU"). It is being standardized as FIPS 206 to complement ML-DSA.
- **Mathematical Foundation**: NTRU lattices (lattices defined by polynomial rings Z[x]/(x^n + 1)), Short Integer Solution (SIS) problem, GPV framework (use a good basis/trapdoor of a lattice to sample short vectors from cosets), Gaussian sampling using Gram-Schmidt orthogonalization in the Fourier domain, recursive fast Fourier sampling (the "FALCON tree"). Security reduces to the hardness of finding short vectors in NTRU lattices (a specific family of ideal lattices).
- **RSA Relevance**: FALCON's reliance on NTRU lattices (a specific class of ideal lattices) makes it potentially more vulnerable to algebraic attacks than ML-DSA's module lattices. Any advance in algorithms for ideal lattice problems could affect both FALCON and potentially improve classical factoring via number field sieve improvements (which also exploit ideal lattice structure in number fields). The connection between ideal lattices and algebraic number theory is direct: the ring of integers of a number field forms a lattice, and its ideal structure is what NFS exploits for factoring. Progress on one front could catalyze progress on the other.
- **Status**: Active standardization (FIPS 206 in development)
- **Open Questions**: Are NTRU/ideal lattices significantly easier than general module lattices? Could advances in algebraic number theory (which connects to both NFS factoring and ideal lattice problems) simultaneously threaten FALCON and improve factoring? Will FALCON's Gaussian sampling implementation be vulnerable to side-channel attacks in practice? How does FALCON compare to ML-DSA in post-quantum hybrid deployments?

---

### NIST IR 8547: Transition Timeline for Deprecating RSA and Classical Public-Key Cryptography
- **Authors**: NIST
- **Year**: 2024 (initial publication), with timeline extending to 2035
- **Source**: [NIST IR 8547](https://csrc.nist.gov/pubs/ir/8547/ipd)
- **Core Idea**: NIST IR 8547 lays out the concrete timeline for deprecating RSA and other quantum-vulnerable algorithms. Key milestones: RSA, ECDSA, EdDSA, and DH/ECDH should not be used in new systems after 2030 and must be fully deprecated by 2035. For National Security Systems (NSS), the timeline is accelerated: PQC adoption required by 2027 (see CNSA 2.0). For TLS 1.3 specifically, post-quantum key exchange should be deployed by 2030. The document acknowledges that the transition will be gradual, with hybrid modes (combining classical and PQC algorithms) serving as a bridge. It also recognizes that some systems (embedded, legacy) may not meet these deadlines, requiring risk-based exemption processes.
- **Mathematical Foundation**: The timeline is based on threat modeling: estimated time to cryptographically relevant quantum computer (CRQC) vs. data sensitivity lifetime. If data must remain confidential for Y years and a CRQC arrives in X years, then data encrypted today with RSA is at risk if X < Y + migration_time. Current estimates for CRQC range from 2030 (aggressive) to 2045+ (conservative), with most expert assessments centering around 2035-2040 for breaking RSA-2048. The precautionary principle drives the earlier deprecation dates.
- **RSA Relevance**: This is the definitive institutional acknowledgment that RSA has an expiration date. The 2035 deadline means RSA has at most ~10 years of legitimate use remaining, regardless of when quantum computers actually arrive. Importantly, the timeline also creates a window: between now and 2035, RSA remains the dominant deployed public-key algorithm, meaning any classical factoring breakthrough during this period would have maximum impact. The transition timeline also reveals the enormous scale of the migration challenge -- billions of certificates, millions of embedded systems, decades of legacy software -- suggesting that RSA will persist in practice well beyond its official deprecation date.
- **Status**: Active policy (published by NIST, being adopted by industry and government)
- **Open Questions**: Will the 2035 deadline be met, or will practical constraints force extensions? What happens to long-lived embedded systems (industrial control, automotive, medical devices) that cannot be easily updated? Will a classical factoring breakthrough accelerate the timeline? How will certificate authorities and PKI infrastructure handle the transition (dual certificates, hybrid modes, backward compatibility)?

---

### CNSA 2.0: NSA's Commercial National Security Algorithm Suite
- **Authors**: NSA (National Security Agency)
- **Year**: 2022 (announced), 2024-2025 (implementation guidance updates)
- **Source**: [NSA CNSA 2.0 FAQ](https://media.defense.gov/2022/Sep/07/2003071836/-1/-1/0/CSI_CNSA_2.0_FAQ_.PDF)
- **Core Idea**: CNSA 2.0 is the NSA's quantum-resistant cryptographic algorithm suite for National Security Systems (NSS). It mandates specific PQC algorithms and an aggressive transition timeline: all new NSS system acquisitions must be PQC-compliant by January 2027 (key agreement) and 2028 (digital signatures). The suite specifies: ML-KEM-1024 for key encapsulation, ML-DSA-87 for general-purpose signatures, XMSS/LMS for firmware/software signing. Notably, CNSA 2.0 goes beyond NIST's general timeline by requiring the highest security parameter sets (ML-KEM-1024, not 768; ML-DSA-87, not 65) and by setting earlier deadlines than the civilian sector. The NSA's urgency signals its assessment of the quantum threat timeline.
- **Mathematical Foundation**: Same underlying mathematics as NIST PQC standards (MLWE for ML-KEM, lattice-based for ML-DSA) but with higher security parameters corresponding to approximately 256-bit classical security / 128-bit quantum security. XMSS (eXtended Merkle Signature Scheme) and LMS (Leighton-Micali Signature) are hash-based stateful signature schemes with security depending only on hash function properties -- the most conservative possible assumption.
- **RSA Relevance**: The NSA's accelerated timeline (2027 for key agreement) reflects an insider assessment that the quantum threat may materialize sooner than public estimates suggest. This is significant because the NSA has historically been ahead of the public on cryptographic threats (e.g., they were reportedly aware of differential cryptanalysis before it was publicly discovered). The choice of highest security parameters also suggests the NSA is hedging against both quantum attacks and potential classical improvements in lattice algorithms. For the factoring question: if the NSA is treating RSA as already compromised for long-term secrets (via harvest-now-decrypt-later), this implicitly assigns a non-trivial probability to classical or near-term quantum factoring breakthroughs.
- **Status**: Active policy (mandatory for U.S. National Security Systems)
- **Open Questions**: Does the NSA's aggressive timeline reflect classified knowledge about quantum computing progress, classical factoring advances, or both? Will the 2027 deadline be achievable for all NSS systems? How will legacy NSS systems that cannot be upgraded be handled? Does the mandate for maximum security parameters (1024, 87) reflect concern about near-term algorithmic advances against lattice problems?

---

### Harvest-Now-Decrypt-Later: The Retroactive Quantum Threat
- **Authors**: Various (concept widely discussed in cryptographic community); Michele Mosca (quantum risk assessment framework)
- **Year**: 2015-present (concept), with increasing urgency
- **Source**: [Mosca, "Cybersecurity in an Era with Quantum Computers"](https://doi.org/10.1109/MSP.2018.3761723), [ETSI QSC White Paper](https://www.etsi.org/technologies/quantum-safe-cryptography)
- **Core Idea**: Nation-state adversaries are collecting and storing encrypted network traffic today with the intention of decrypting it in the future when quantum computers become available. This "harvest-now-decrypt-later" (HNDL) strategy means that data encrypted with RSA or ECDH today is already at risk if: (a) the data's sensitivity outlasts the time until quantum computers arrive, and (b) the adversary has the storage capacity and network access to capture relevant traffic. For intelligence data, diplomatic communications, trade secrets, and personal health records -- all of which may need to remain confidential for 20-50+ years -- the HNDL threat makes the effective deadline for PQC migration not "when quantum computers arrive" but "now." This is the most urgent practical driver for PQC adoption.
- **Mathematical Foundation**: Mosca's inequality: if X = time until CRQC, Y = data sensitivity lifetime, Z = migration time, then data is at risk when X < Y + Z. For sensitive data captured today: Y = remaining sensitivity (e.g., 30 years for classified data), Z is already elapsed (data is captured), so risk materializes when X < Y. Current estimates: X is approximately 10-20 years, Y can be 30-50 years for classified/health/financial data, Z is 5-10 years for full ecosystem migration. The math clearly shows the risk is already materialized for long-lived secrets.
- **RSA Relevance**: HNDL fundamentally changes the RSA threat timeline. The question is no longer "when will someone be able to factor RSA keys?" but rather "when was the data encrypted?" For traffic encrypted with RSA key exchange in 2020, the adversary already has the ciphertext; they just need to factor the RSA key (or the session key exchange, if RSA or ECDH was used) at any point in the future. This retroactive vulnerability means that even if quantum computers don't arrive until 2040, data from the 2020s is still compromised. It also means a classical factoring breakthrough would be equally devastating -- the stored traffic doesn't care whether it's decrypted by Shor's algorithm or by a classical advance.
- **Status**: Active threat (widely acknowledged by intelligence agencies, NIST, NSA, GCHQ, BSI)
- **Open Questions**: How much encrypted traffic is being stored by nation-state adversaries? What fraction uses RSA or ECDH key exchange (vs. forward-secret key exchange with ephemeral keys)? Does the use of TLS 1.3 with ephemeral ECDHE mitigate the HNDL threat (yes for the key exchange, but not for the certificates)? Is the storage cost ($10-20/TB/year) low enough to make mass collection economically feasible for large nations? How do we assess risk for data with uncertain sensitivity lifetimes?

---

### Lattice-Based Cryptography Security: The MLWE Foundation
- **Authors**: Oded Regev (LWE, 2005), Adeline Langlois, Damien Stehle (Module-LWE, 2015), Chris Peikert (surveys)
- **Year**: 2005 (LWE), 2015 (Module-LWE), ongoing
- **Source**: [Regev, "On Lattices, Learning with Errors, Random Linear Codes, and Cryptography"](https://arxiv.org/abs/2401.03703) (updated 2024), [Peikert, "A Decade of Lattice Cryptography"](https://eprint.iacr.org/2015/939)
- **Core Idea**: The security of ML-KEM and ML-DSA (and thus the majority of the PQC ecosystem) rests on the hardness of the Module Learning with Errors (MLWE) problem. MLWE generalizes the original LWE problem (Regev, 2005) to module lattices, providing a balance between the strong worst-case hardness guarantees of unstructured LWE and the efficiency of Ring-LWE. Regev's original breakthrough showed that LWE is at least as hard as solving worst-case lattice problems (approximate GapSVP and SIVP), which are believed to be hard for both classical and quantum computers. However, the concrete security of MLWE at practical parameters depends on the best known attacks, which are primarily lattice sieving (BKZ with sieving subroutines) and dual attacks. The security margin -- the gap between proven hardness and best known attacks -- is a subject of ongoing research.
- **Mathematical Foundation**: Learning with Errors: given (A, b = A*s + e) where A is random in Z_q^{m x n}, s is secret in Z_q^n, and e is drawn from a discrete Gaussian with parameter sigma, find s. Worst-case to average-case reduction: breaking LWE implies solving approximate GapSVP_{gamma} on n-dimensional lattices, where gamma = O(n/alpha) for error rate alpha. Module-LWE extends this to modules over polynomial rings Z_q[x]/(f(x)), with security parametrized by module rank d and polynomial degree n. Best known attacks: primal BKZ (block size beta determines cost as 2^{0.292*beta} via sieving), dual attacks (similar asymptotics), and algebraic attacks exploiting ring structure.
- **RSA Relevance**: If lattice problems (specifically MLWE at PQC parameter sizes) turn out to be easier than currently believed, the PQC transition becomes far more urgent and complex -- the "safe harbor" of lattice-based cryptography would be compromised, potentially leaving only hash-based (SLH-DSA) and code-based (HQC) schemes as alternatives. Conversely, algorithmic advances in lattice problems could also improve NFS-based factoring (which uses lattice reduction in the relation-finding and linear algebra phases) and other approaches to breaking RSA. The shared mathematical substrate (lattices, algebraic number theory) means that progress on lattice algorithms is relevant to both PQC security and RSA vulnerability. This creates a peculiar situation: improvements in attacking PQC could simultaneously improve classical RSA attacks, threatening both the old and new cryptographic paradigms.
- **Status**: Active research (MLWE hardness is well-studied but not proven unconditionally; best known attacks are well-understood but the concrete security margin at PQC parameters is debated)
- **Open Questions**: Are there undiscovered algebraic attacks on Module-LWE that exploit the ring structure? Will advances in lattice sieving algorithms (especially GPU-accelerated implementations) erode the security margin faster than expected? Does the worst-case to average-case reduction for LWE extend tightly to Module-LWE? Could quantum algorithms for lattice problems (beyond Grover speedup of sieving) provide super-quadratic advantage? What is the actual concrete quantum query complexity for breaking ML-KEM-768?

---

## Synthesis: The 10-Year Window Where RSA Remains Vulnerable

The post-quantum landscape reveals a critical vulnerability window that is already open and will persist for at least a decade:

**The Timeline Squeeze**:
- **Now (2025)**: RSA is ubiquitous. PQC standards are finalized but adoption is nascent. HNDL collection is ongoing.
- **2027**: CNSA 2.0 deadline for NSS key agreement. Early adopters deploying ML-KEM in TLS 1.3.
- **2030**: NIST target for PQC in TLS 1.3. RSA should not be used in new systems.
- **2035**: RSA fully deprecated per NIST IR 8547. Legacy systems still running RSA.
- **2035-2040**: Most expert estimates for CRQC capable of breaking RSA-2048 (1,000-4,000 logical qubits with error correction).
- **2040+**: Long tail of RSA in embedded/legacy systems. HNDL-collected traffic still being decrypted.

**Why This Window Matters for Factoring Research**:

1. **Maximum Impact Period**: Any classical factoring breakthrough in the 2025-2035 window would catch the cryptographic ecosystem at maximum vulnerability -- RSA is still dominant, PQC migration is incomplete, and the "insurance" of quantum resistance is not yet universally deployed.

2. **HNDL Retroactivity**: Even after the transition is complete, HNDL-collected traffic from the pre-PQC era remains vulnerable. A factoring breakthrough in 2040 would still compromise communications from the 2010s-2020s. The retroactive nature of the threat means the vulnerability window extends backward in time, not just forward.

3. **Lattice-RSA Correlation Risk**: The shared mathematical foundations between lattice problems (PQC security) and algebraic number theory (RSA factoring via NFS) create a correlated risk: a breakthrough in lattice algorithms could simultaneously weaken PQC standards and improve classical factoring. This "double threat" scenario -- where both the old and new cryptographic paradigms are compromised -- is the nightmare scenario for the cryptographic community.

4. **The Migration Challenge**: The sheer scale of the RSA-to-PQC transition (billions of TLS certificates, millions of embedded systems, decades of legacy software, entire PKI hierarchies) ensures that RSA will persist in practice well beyond official deprecation dates. Every system that delays migration is a system that remains vulnerable to both quantum and classical factoring advances.

5. **Intelligence Asymmetry**: The NSA's aggressive CNSA 2.0 timeline (2027) compared to NIST's civilian timeline (2035) suggests that intelligence agencies may have information about the factoring threat (classical or quantum) that is not public. This asymmetry should inform the urgency assigned to factoring research, both offensive and defensive.

The bottom line: **RSA is living on borrowed time, and the loan period is approximately 10 years.** During this window, the world's most sensitive communications (past and present) remain protected only by the assumption that factoring large semiprimes is computationally infeasible. Any advance -- classical, quantum, or quantum-inspired -- that undermines this assumption would have consequences that are retroactive, pervasive, and potentially catastrophic. The post-quantum transition is necessary but not sufficient; only completing the transition eliminates the risk.
